
from http://9legacy.org/patch.html

libventi-noarchive 	/sys/src/libventi 	Define VtEntryNoArchive constant. 	David du Colombier
http://9legacy.org/9legacy/patch/libventi-noarchive.diff

libventi-redial 	/sys/src/libventi 	Implement vtreconn and vtredial functions. 	David du Colombier
http://9legacy.org/9legacy/patch/libventi-redial.diff

libventi-sha1 	/sys/src/libventi 	Implement vtsha1 and vtsha1check functions. 	David du Colombier
http://9legacy.org/9legacy/patch/libventi-sha1.diff

I am not sure if I need this patch. I cannot find sys/lib/dist/pc directory on plan9front
dist-venti-bloom 	/sys/lib/dist/pc/inst 	Add Venti Bloom filter configuration. 	David du Colombier

<mycroftiv> you dont need that bloom filter config, that is for making the installer
            set up bloom filter when it is doing a clean system install

from https://marc.info/?l=9fans&m=140234533706362&w=2
> Other things to note:
>
> - make sure you mirror or backup the venti disk or else you
>   may lose the only copy of a block!
> - performance will likely be poor. For better performance you
>   may want to keep venti index on a separate (flash) disk.

from https://marc.info/?l=9fans&m=140221685729283&w=2
> if you have big files you don't want to keep in venti use chmod -t on them
> to stop archiving (keep the file in fossil only). This means they are not backed up
> in venti but I find it helpful for things like downloaded ISO files where they
> can be easily regenerated.

for venti status: hget http://ventiserver/storage

from https://marc.info/?l=9fans&m=136327650812842&w=2
Well. Venti does block-level deduplication. If you have 100 MB of data
in dir A and move it to dir B, you're not going to duplicate that storage
(you'll get new blocks for the directory, of course, but that's both
minimal and unavoidable).

from https://marc.info/?l=9fans&m=132680552426120&w=2
> So it seems sufficient to backup my arenas, am I
> right?
Yes, exactly, I haev done this several times.
It might take a few hours and some studying of manuals
but the arenas are all you need.

from https://marc.info/?l=9fans&m=132680563426154&w=2
> I've backed up all my *active* arenas in to another disk, just to be
> safe. The man page says that the index and the bloom filter may be
> rebuilt if lost. So it seems sufficient to backup my arenas, am I
> right?
Yes, you can rebuild the index and the Bloom filter
with 'venti/buildindex -b'.

David du Colombier's backup process
      https://marc.info/?l=9fans&m=147750604517288&w=2
> I see several threads about how people are cloning their Venti
> servers to remote Venti servers as a means of creating a backup.

Personally, I use three different ways of replicating the
content of my Venti file servers.

1. The first method is to use venti/mirrorarenas to replicate
the content of a Venti arena partition to another hard disk.

venti/mirrorarenas /dev/sdE0/arenas /dev/sdE1/arenas

This method requires two hard disks and two identical arena partitions.

2. The second method is to copy blocks from a Venti arena partition
to a Venti server running on another machine, accessed through the
local network.

You'll find an example of script using this method here:

http://9legacy.org/www.9legacy.org/9legacy/tools/backup/readwrite

This method requires another machine running Venti. The partition
layout could be different and the Venti servers could be running
on different operating systems.

3. The third method is to copy blocks from a Venti arena partition
to arena files, using venti/rdarena. Then you can copy the arena files
to external media like Blu-ray discs or hard disks.
The restoration is done to a running Venti using venti/wrarena.

You'll find backup and restore script examples here:

http://9legacy.org/www.9legacy.org/9legacy/tools/backup/read
http://9legacy.org/www.9legacy.org/9legacy/tools/backup/write

This method require external media to store the arena files.

For example, I use all these three methods to replicate the data
of my main file server. Consequently, the data are replicated on:

 - two machines (automatically)
 - two hard disks of each machine (automatically)
 - some external hard disks (manually)

from https://marc.info/?l=9fans&m=141829654013932&w=2
> I guess there are also people using fossil+venti on
> p9. Are those happy?
I'm using Fossil and Venti on Plan 9 and I'm quite happy.
On my typical setup, I've two arenas partitions mirrored
on two hard disks using venti/mirrorarenas.
My main file server is mirrored to another file server
using venti/wrarena every night.
I'm regularly dumping arenas to backup hard disks
and blu-ray using venti/rdarena.
I don't really store any long-term stuff on my Linux
machines. I just copy the files to the Plan 9 file
servers using v9fs.


from https://marc.info/?l=9fans&m=132579812309672&w=2
If fossil is setup to dump to venti then it needs venti to
work at all. Fossil is a write cache, so, just after the dump
at 4am fossil is empty and consists only of a pointer to the
root of the dump in venti; all reads are then satisfied from
venti alone, until some data is written.

from https://marc.info/?l=9fans&m=132568421404140&w=2
Don't forget you can always grow your venti by adding more arenas
partitions. More indexes can be added also - rebuilt from your
existing arena partitions).
Even fossil can be grown though you will need a new bigger
partition or grow the existing one using fs(3), this can then
be refreshed from a venti snapshot.

from https://marc.info/?l=9fans&m=132489954630387&w=2
Fossil is a write cache for venti - "the blocks in the archival
snapshots are moved to Venti". Read requests are proxied for venti.
Recovery means initializing the filesystem using the given vac scrore,
with no implied data transfer - blocks are copied-on-write.

from https://marc.info/?l=9fans&m=131314281221012&w=2
>Remove -V to open and srv if needed, then
>add the following line to enable automatic
>snapshots:
>
>fsys main snaptime -s 60 -a 0530 -t 2880
>
>(snapshot at 05:30, temporary snapshot each
> hour and discarded after two days)

below is irrelevant - bug fixed long time ago
from https://marc.info/?l=9fans&m=131307665628025&w=2
> Beware: there is currently a bug in fossil which can cause
> it to deadlock under load if ephemerial snapshots are enabled.
>
> This I reccomend you use a configuration more like:
>
> 	fsys main snaptime -s none -a 0530 -t none
<joe9> https://marc.info/?l=9fans&m=131334453728485&w=2 Do you know if this bug has been fixed? Or, is it recommended not to bother with ephemeral snapshots still.  [13:13]
<Fish> I fixed it few days after this message  [14:19]
<Fish> richard miller and I fixed all the known fossil issues in 2011

David's setup
<joe9> the 80GB disks, I have them mirrored using mkfs. They do not change much other than when I do an upgrade or some such.  [11:26]
<Fish> I usually set up five disks  [11:27]
<Fish> 0 is an SSD with fossil for the system
<Fish> 1 is fossil for the storage
<Fish> 2 is venti index + bloom  [11:28]
<Fish> 3 and 4 are venti arenas, mirrored
<Fish> and this setup is itself mirrored on another machine
<Fish> I've also a fossil on the other machine, but it's not set up
<Fish> it's just in case the first machine burn, so I can quickly switch to the new machine
<Fish> index is 1/20 of arenas I think
<Fish> for the bloom, I always set it to its maximum value : 512 MB  [12:30]
<Fish> disk 1 only contains the fossil partition, so it contains no data (the data are on venti)
<Fish> and disk 2 only contains index and bloom, which can be rebuilt anytime

from https://marc.info/?l=9fans&m=130881885622034&w=2 regarding stability

from https://marc.info/?l=9fans&m=129503282218163&w=2
what i understood the op to mean was that venti arenas are
garbage collectable.  this is not true.  arenas are write once,
so it follows they must be impossible to garbage collect.
venti is the block motel.  blocks check in, but they never check out.

mirror venti to another
http://www.cespedes.org/blog/90/how-to-mirror-one-venti-to-another

to verifyarena on both.

rebuilding or recovering fossil from venti arenas:
http://sources.postnix.us/contrib/steve/doc/Venti-rescue.pdf

installing venti patches (using drawterm)

	cd /
	ape/patch -p0 </mnt/term/home/j/dev/apps/plan9/custom/9front-patches/venti/libventi-noarchive.diff
	ape/patch -p0 </mnt/term/home/j/dev/apps/plan9/custom/9front-patches/venti/libventi-redial.diff
	ape/patch -p0 </mnt/term/home/j/dev/apps/plan9/custom/9front-patches/venti/libventi-sha1.diff
	ape/patch -p1 </mnt/term/home/j/dev/apps/plan9/custom/9front-patches/venti/allow-fossil-p9sk1-authentication.patch
	ape/patch -p1 </mnt/term/home/j/dev/apps/plan9/custom/9front-patches/venti/stop-fossil-venti-servers.patch
	cd /sys/src/libventi
	mk clean
	mk install
	mk clean

installing fossil
   ;# hg clone https://bitbucket.org/mycroftiv/fossilnewventi
   This is the new version of fossil using libventi instead of liboventi

	cd /mnt/term/home/j/dev/apps/plan9/custom/src/fossilnewventi/
        INSTALL

configuring crypt+fossil+venti
    # disk to be used: WD Green 2 TB - sdE2 -- venti + fossil daily backup
    # delete any existing partition on sdE2, using disk/fdisk /dev/sdE2/data, d p1, w, q

        disk/mbr /dev/sdE2/data
        disk/fdisk -baw /dev/sdE2/data
        ls /dev/sdE2
        /dev/sdE2/ctl
        /dev/sdE2/data
        /dev/sdE2/plan9
        /dev/sdE2/raw
        disk/prep -w -b -a^(fossil arenas bloom isect) /dev/sdE2/plan9
        no plan9 partition table found
        fossil 624956068
        arenas 3124780340
        isect 156239018
        bloom 1048573

        <mycroftiv> joe9: however big the bloom filter is, it all gets loaded
        into ram, the partitioner caps it at 512mb but im pretty sure it will
        allocate that full 512mb by default on a hdd as large as you are using
        Hence, use the partition sizes reported by the p command of disk/prep
        to change the bloom to 64mb.

        On the 2TB disk, disk/prep reported:

        empty                    0 2             (2 sectors, 1.00 KB)
        fossil                   2 624956070     (624956068 sectors, 298.00 GB)
        arenas           624956070 3749736410    (3124780340 sectors, 1.45 TB)
        isect           3749736410 3905975428    (156239018 sectors, 74.50 GB)
        bloom           3905975428 3907024001    (1048573 sectors, 511.99 MB)
        empty           3907024001 3907024002    (1 sectors, 512 B )

        Need to size these better. Per
        http://plan9.bell-labs.com/wiki/plan9/Setting_up_Venti/index.html
        , 5% as much index space as you have arena space.

        Using the above sizes and restricting bloom to 64m, I came up with the below
        echo 'a fossil 2 2+298g
        a bloom . .+65m
        a isect . .+75g
        a arenas . $-1
        w
        p
        q' | disk/prep -b  /dev/sdE2/plan9

using cryptsetup on /dev/sdE2/fossil, /dev/sdE2/arenas, /dev/sdE2/isect, /dev/sdE2/bloom:
	# From http://fqa.9front.org/fqa4.html , bottom of the page: format the partitions
	disk/cryptsetup -f /dev/sdE2/fossil /dev/sdE2/arenas /dev/sdE2/isect /dev/sdE2/bloom
        # use this command to figure out the ctl activation commands. Use this when starting up the disk server:
	disk/cryptsetup -o /dev/sdE2/fossil /dev/sdE2/arenas /dev/sdE2/isect /dev/sdE2/bloom
        # to activate, either echo the output from the above command to /dev/fs/ctl or run the below command
	disk/cryptsetup -i /dev/sdE2/fossil /dev/sdE2/arenas /dev/sdE2/isect /dev/sdE2/bloom

venti setup:
      formatting
	venti/fmtarenas arenas-sdE2 /dev/fs/arenas-sdE2
	venti/fmtisect isect-sdE2 /dev/fs/isect-sdE2
	venti/fmtbloom /dev/fs/bloom-sdE2
      initialize venti index sections
	venti/fmtindex /mnt/term/home/j/dev/apps/plan9/custom/9front-patches/venti/using-cryptsetup-and-multiple-instances/venti.conf
      configure
	venti/conf -w /dev/fs/arenas-sdE2 /mnt/term/home/j/dev/apps/plan9/custom/9front-patches/venti/using-cryptsetup-and-multiple-instances/venti.conf
        venti/venti -c /dev/fs/arenas-sdE2
        venti=192.168.0.5

fossil setup:
        fossil/flfmt /dev/fs/fossil-sdE2
        # also, creating the root directory in /n/fossil here to avoid having to do it later
        # in this instance, it is backups-sdE2
        fossil/conf -w /dev/fs/fossil-sdE2 /mnt/term/home/j/dev/apps/plan9/custom/9front-patches/venti/using-cryptsetup-and-multiple-instances/initfossil.conf
        fossil/fossil -f /dev/fs/fossil-sdE2
        # add in my linux users, do not bother with matching the id with the linux uid.
        # It seems to work with just names on fossil
        # on linux, uid = 101, name = joe
        # on 9front fossil, id = joe, name = joe
        # on 9front authentication server, user = joe
        fossil/conf -w /dev/fs/fossil-sdE2 /mnt/term/home/j/dev/apps/plan9/custom/cirno/init-fossil-users.conf
        fossil/fossil -f /dev/fs/fossil-sdE2
        # for automatic snapshots at 0500, add this to the below fossil.conf: fsys main snaptime -a 0500
	fossil/conf -w /dev/fs/fossil-sdE2 /mnt/term/home/j/dev/apps/plan9/custom/9front-patches/venti/using-cryptsetup-and-multiple-instances/fossil.conf
        ls /srv
        mount -c /srv/fossil-sdE2 /n/fossil-sdE2

disk server startup:
        # do not need these commands: diskparts does it.
        #  disk/fdisk -p /dev/sdE2/data > /dev/sdE2/ctl
        #  disk/prep -p /dev/sdE2/plan9 > /dev/sdE2/ctl

        # added these to /cfg/$sysname/termrc of cirno (my disk server)
	# activate the cryptsetup partitions, the echo command is from the above disk/cryptsetup -o
	echo 'crypt fossil-sdE2 /dev/sdE2/fossil 4ED863CA244CAD00FDDBA6A9EC8774A627389E509DA7F8BB2253331B80D17D05'>>/dev/fs/ctl
	echo 'crypt arenas-sdE2 /dev/sdE2/arenas 314E6D9E4815054E156B811E712BFC4E57D3C829B9D283DE43B2E9EDFFD224F1'>>/dev/fs/ctl
	echo 'crypt isect-sdE2 /dev/sdE2/isect 69E1E73B3E9FB9D674C2103E1C23659A590A8FEB6424E18AFF0E945F9292142B'>>/dev/fs/ctl
	echo 'crypt bloom-sdE2 /dev/sdE2/bloom 69E1E73B3E9FB9D674C2103E1C23659A590A8FEB6424E18AFF0E945F9292142B'>>/dev/fs/ctl
        venti=192.168.0.5
        venti/venti -c /dev/fs/arenas-sdE2
        fossil/fossil -f /dev/fs/fossil-sdE2
        ls /srv
        mount -c /srv/fossil-sdE2 /n/fossil-sdE2
        # or, add the below to fossil.conf
        echo 'listen -N tcp!*!564' >>/srv/fscons-sdE2

drawterm startup (can add this to profile):
        ls /srv
        mount -c /srv/fossil-sdE2 /n/fossil-sdE2

to stop the fossil and venti servers (added to fshalt by the above patch):
        venti/sync
        echo fsys all halt >>/srv/fscons-sdE2

to stop venti manually:
        kill venti | rc
        kill fossil | rc

to stop cryptsetup disks:
        echo 'del fossil-sdE2'>>/dev/fs/ctl
        echo 'del arenas-sdE2'>>/dev/fs/ctl
        echo 'del isect-sdE2'>>/dev/fs/ctl
        echo 'del bloom-sdE2'>>/dev/fs/ctl

to copy data from existing machine to the new fossil system:
        mkdir /n/fossil-sdE2/backups/destination
        disk/mkfs -s /mnt/term/src/ -d /n/fossil-sdE2/backups/destination /sys/lib/sysconfig/proto/allproto
        echo fsys all snap -a >>/srv/fscons-sdE2
        venti/sync

linux setup if you plan on using plan9port to mount the fossil on linux
    # 9pfuse is more locked down and does not sudo but has issues with chmod, etc.
    put these in the shell profile:
    PLAN9=/home/j/dev/apps/plan9/plan9port; export PLAN9
    PATH=$PATH:$PLAN9/bin; export PATH
    # ensure that the below line to set authdom and auth is in $PLAN9/ndb/local:
    authdom=9front auth=cirno
    # cat mypasswords
    echo 'key dom=9front proto=p9sk1 role=client user=joe !password=test6666' >mypasswords

to mount the fossil on linux
    factotum -n
    9p write -l factotum/ctl <mypasswords
    # below command does not work without the PLAN9 prefix
    srv -a -k user=joe cirno:564 fossil
    namespace
    # 9pfuse has issues changing permissions, use the patch from
    # https://github.com/9fans/plan9port/issues/81
    9pfuse unix\!$(namespace)/fossil /home/j/fossil-backups
    # or, if you prefer using mount below
    sudo mount -t 9p -o unix,trans=unix,uname=$USER,dfltuid=$(id -u),dfltgid=$(id -g) $(namespace)/fossil /home/j/fossil-backups

to mount the fossil server on linux using 9pfuse:
        9pfuse is more locked down and does not sudo
        fix for 9pfuse chmod issue https://github.com/9fans/plan9port/issues/81
        use -a main to see the snapshots (the whole tree)
        PLAN9=/home/j/dev/apps/plan9/plan9port /home/j/dev/apps/plan9/plan9port/bin/9pfuse 'tcp!cirno!564' /tmp/mnt
        disk/mkfs -s /mnt/term/src/ -d /n/fossil-sdE2/backups/destination /sys/lib/sysconfig/proto/allproto
        TODO how to do this from linux? -- echo fsys all snap -a >>/srv/fscons-sdE2
        TODO how to do this from linux? -- venti/sync

to mount the fossil server on linux using v9fs with permission checking:
        # sudo mount -t 9p -o 'trans=tcp,port=564,uname=joe,dfltuid=1001,dfltgid=1000,access=1001' 192.168.0.5 /home/j/fossil-backups
        # below seems to work well
        # from https://marc.info/?l=9fans&m=149673010624331&w=2
        sudo mount -t 9p -o unix,trans=unix,uname=$USER,dfltuid=$(id -u),dfltgid=$(id -g) $($PLAN9/bin/namespace)/fossil /home/j/fossil-backups
        disk/mkfs -s /mnt/term/src/ -d /n/fossil-sdE2/backups/destination /sys/lib/sysconfig/proto/allproto
        TODO how to do this from linux? -- echo fsys all snap -a >>/srv/fscons-sdE2
        TODO how to do this from linux? -- venti/sync

to get user authentication to work
    1. set service=cpu in plan9.ini, ensure that there is a newline at the end
    2. startup scripts:
        cd cirno;
        cp cpurc /cfg/$sysname/cpurc;
        cp cpustart /cfg/$sysname/cpustart;
        cp termrc /cfg/$sysname/termrc
        cp profile $home/lib/profile
    3. edit /lib/ndb/local, add auth and authdom
    7.4.1 - Configuring an auth server
        add the below line
            ipnet=9front ip=192.168.0.0 ipmask=255.255.255.0 auth=cirno authdom=9front
    3. configure nvram (use the same password here and in #4 below)
        7.3.2 - Configuring nvram
              auth/wrkey
    4. add users
        7.4.2 - Adding users
            auth/keyfs
            auth/changeuser glenda

to run the fossil snapshot
    joe9> Fish, this works, just want to run this by you: on the 9front machine:
    aux/listen1 -tv tcp!*!1000 /bin/rc -xv -c 'venti/sync; /bin/echo fsys all snap -a >>/srv/fscons-sdE1'
    <joe9> on linux, echo "junk string" | socat STDIN TCP4:cirno:1000
    <joe9> not sure if there is a better way of going about it.  [15:09]

to see the size of the fossil disk partition
        con /srv/fscons-sdE2
        fsys main df
        to exit: ctrl-backslash q

create a root directory in /n/fossil to use:
<mycroftiv> fsys main create active/backups username username d775, gotta specify uid and gid   [19:36]
<mycroftiv> you use the /srv/fscons
<mycroftiv> its like con /srv/fscons, type commands, then ctrl-backslash q
<joe9> cool, That worked.

:glenda@cirno:/srv; con /srv/fscons
prompt: fsys main create active/backups glenda glenda d775
prompt: >>> q
:glenda@cirno:/srv; cd /n/fossil


<mycroftiv> do you understand how to trigger snapshots and how to access the archival
            dump?  [19:41]
<joe9> I am reading up the manpages . snaptime for automatic  [19:42]
<mycroftiv> yeah, you can also make a snapshot whenever you want, by connecting to the
            fscons and doing fsys main snap - a  [19:43]
<mycroftiv> when it completes, it will print the rootscore of that snapshot
<mycroftiv> you can also see the msot recent snapshot rootscore if you have all the
            fossil commands installed by doing fossil/last /path/to/fossildrive
<mycroftiv> once you have made snapshots, you can view them by doing something like
            mount /srv/fossil /n/dump main/archive
<mycroftiv> the 'archive' is a separate tree that fossil serves containing all the
            snapshots you have made, organized by date  [19:47]
<mycroftiv> you can also access them by storing the rootscores i mentioned earlier,
            and putting the output of a fossil/last command into a file called like
            foo.vac and using vacfs foo.vac to access it  [19:48]

<joe9> I can get the /n/fossil/backups show up in my serial console (booting 9front on
       serial console)  [19:44]
<joe9> but, with drawterm, I see /n/fossil but nothing below that.
<mycroftiv> thats just a namespace issue im sure  [19:45]
<mycroftiv> drawterm in and do mount -c /srv/fossil /n/fossil
<mycroftiv> your /lib/namespace file doesnt have that mount in it, and when you
            drawterm in you get a new namespace created from what it says in that file
<mycroftiv> you could also add the mount command to your profile, which is also run
            when you drawterm in, as an alternate method  [19:46]
<joe9> cool, that worked. Thanks.

<mycroftiv> so you want like srv -p fscons.diskA, srv -A fossil.diskA for one, and
            diskB for those parameters with the other, i think you get it
<mycroftiv> each fossil will have its own fscons that only knows about that one
<mycroftiv> and all the mount commands, etc will be separate  [19:53]
<mycroftiv> and there will be two sets of fossil processes running
<mycroftiv> similarly with venti, and if this is on the same machine, youll obviously
            have to give them different ports to listen and communicate on  [19:54]
<mycroftiv> so one would be using 17034 for venti data and 8000 for the http
            interface, and the other would be on 17035 and 8001
<mycroftiv> with the fossils configured to know which one they are supposed to talk to
<mycroftiv> then you could use the wrarena/backup stuff we discussed earlier to have
            them synchronize what data is available in each  [19:55]
[mycroftiv]

<joe9> btw, why is the http port needed?  [19:57]
<joe9> or, is it mandatory?  [19:58]
<joe9> I started mine without it and it seemed to startup.
<mycroftiv> its how you control venti and get meta-information out of it
<mycroftiv> and it is probably using the default port
<mycroftiv> i forget if it defaults to using :8000 if you dont specify anything
<mycroftiv> but for instance if you want to do what i was just talking about,
            synchronize the data between ventis, the only way you cna find out the
            information you need about what blocks are written to copy is using that
            port  [19:59]
<mycroftiv> you also kind of need that port to do the fshalt routines safely, because
            it talks on that port to tell venti to make sure to sync to disk
<joe9> I tried mothra <ip>:8000 and it says bad url  [20:00]
<mycroftiv> did you put the http:// first?
<mycroftiv> you can use netstat -n to see if anything is listening on that port, also
                                                                                [20:01]
<joe9> no, I just did it with http:// and it says No status connection refused.
<mycroftiv> so it probably does need to be specifified
<joe9> ok, will do that.
<joe9> btw, how do you pick the port numbers for venti (start from 17034 and keep
       incrementing for additional servers)?  [20:03]
<mycroftiv> 17034 is the default, its entirely up to you what port numbers you use
<joe9> I agree. Just want to check if there is a convention that you use.  [20:04]
<mycroftiv> i only run one venti per box and get my redundancy through multiple nodes
            so that issue never realy comes up for me
<mycroftiv> knowing me, id probably use 17034 and 27034 because when i do have
            multiple copies of a service, i usually do it that way, but its obviously
            just convention  [20:06]
<joe9> that is actually a pretty good idea. and you can patternmatch it using [12]7034
                                                                                [20:10]

port for venti
:glenda@cirno:/lib/ndb; grep venti /lib/ndb/common
tcp=venti port=17034

stopping venti
<joe9> mycroftiv: how do you stop venti? I saw that you are doing venti/sync and nothing more.  [10:06]
<joe9> also, venti does not use the ctl file approach of fossil or other plan9 services. Is there a reason behind that?
<mycroftiv> you really just need to do the sync to make sure everything is written to disk, then you can reboot or kill the processes, based on my understanding
<mycroftiv> the fshalt script that i linked you is based on how the old labs venti/fossil fshalt worked, so i assume it is adequate (maybe a dangerous assumption?)  [10:07]
<mycroftiv> and yeah venti is kinda lame in how it seems to not be very plan9-ish
<mycroftiv> it presents no fs interface and uses that http server approach to controlling it
<mycroftiv> i dont think there is much of a good reason for that, i think maybe the venti authors had ambitions it would be a general-unix program more than staying within plan 9  [10:08]
<mycroftiv> making venti present an fs interface as an alternative to the http server would be a good project actually  [10:11]
<mycroftiv> im actually surprised nobody ever bothered to do it, it wouldnt be very hard
<joe9> I want to change the venti configuration. I did venti/sync, the fsys all halt to /srv/fscons. But, I find multiple venti processes in ps. Any suggestions for killing venti?
<mycroftiv> try typing 'kill venti' - it should output all those processes. so kill venti| rc should work im pretty sure  [10:12]
<mycroftiv> venti and fossil both create a lot of subprocesses  [10:13]
<mycroftiv> also you probably need to kill off the fossil too, i doubt if you kill the venti, reconfigure and restart, that the fossil will reconnect - maybe it will, that might be the point of the venti-redial patch from 9legacy tho
[mycroftiv]

sizing bloom partition to 65MB instead of 64MB
<joe9> I have bloom partition   bloom            624951298 625082370     (131072 sectors, 64.00 MB)
<joe9> but, :glenda@cirno:/srv; 	venti/fmtbloom /dev/fs/bloom-sdE2
<joe9> warning: size not a power of 2; only using 32MB
<joe9> fmtbloom: using 32MB, 32 hashes/score, best up to 5,965,232 blocks
<mycroftiv> that does seem a bit strange, measuring size is always a bit tricky, probably make the bloom a tiny bit bigger and it will use all of it  [10:23]
<mycroftiv> surprises me a little because when i do use bloom, the .+64m has seemed to size it correctly  [10:24]
<joe9> I can try +65m
<mycroftiv> might as well, i doubt it will mess anything up to waste that mb  [10:25]
[mycroftiv]

starting fossil server
<joe9> but, I am not sure how to figure out the fossil address.
<joe9> when I do netstat -n, I do not see fossil service running
<mycroftiv> you need to make fossil start listening in the fscons  [11:33]
<mycroftiv> thats just like listen tcp!*!564  [11:35]
<mycroftiv> i think the -N flag would let you connect without auth
<mycroftiv> you could also run an exportfs listener outside the fossil console, that would work too  [11:36]

to send fossil commands from linux
https://9p.io/wiki/plan9/9p_services_using_srv,_listen,_exportfs,_import/
joe9> after I am done copying the files to the 9pfuse mounted directory, I want to tell fossil to do a snapshot.  [13:23]
<mycroftiv> um what you might be able to do is run an exportfs listener that exports your /srv directory and then just echo to the srv file from linux - never tried that though, but in theory i think it should work  [14:03]
 <joe9> cool, Thanks. will try it after the install

check the fossil partition size
<joe9> mycroftiv: how do you check the size used by fossil? I tried echo fsys all df | con -l /srv/fscons-sdE2 and also echo fsys all df >>/srv/fscons-sdE2  [23:37]
<joe9> both return nothing.
<mycroftiv> con /srv/fscons. then within the fscons do fsys main df.   [00:37]
<mycroftiv> the output is *inside* the fscons
<mycroftiv> prompt: fsys main df
<mycroftiv> 	main: 38,666,240 used + 16,039,813,120 free   [00:38]
<mycroftiv> = 16,078,479,360 (0.2% used)
[mycroftiv]

performance:
<joe9> mycroftiv: the fossil to venti snapshot process is not that fast. Do you notice it too? It took me an hour to get through 1GB.  [08:25]
<joe9> it could be the encrypt too that is slowing it down.  [08:30]
<mycroftiv> i think a lot of that has to be the fs/encrypt layer  [16:34]
<mycroftiv> snapshot is much faster than that for me
<mycroftiv> not blazing speed, but 1gb would be more like 10 min rather than an hour  [16:35]

authentication:
<joe9> I read through the nemo book about factotum and authsrv. I want to enable user permission checking when I mount the fossil+venti on linux.  [18:29]
<joe9> I would need authsrv on the disk server (not on the linux machine ), correct?
<joe9> then, I would need factotum running on both the disk server and the linux machine.
<joe9> but, I cannot figure out how the keys should be setup on each machine. Just want to check if you have any notes or observations about that.  [18:30]
<mycroftiv> theres kind of a meta-issue
<joe9> I understand the uname and users and groups in fscons
<mycroftiv> p9p factotum only does old p9sk1 auth, not dp9ik
<mycroftiv> and p9sk1 is not actually secure any more  [18:31]
<mycroftiv> so if you are looking for a setup that is actually secure against untrusted/potentially malicious people on the network, p9sk1 just doesnt cut it
<mycroftiv> you can say its 'better than nothing' which is true, but you should be aware of this before possibly making use of it  [18:33]
<joe9> ok, Thanks.  [19:52]

permissions:
check for details https://github.com/9fans/plan9port/issues/96
<joe9> Fish, just want to run this by you. I am mounting the fossil+venti fs on linux. Is there any way to get the linux user permissions and fossil users to work?  [21:01]
<joe9> for uname, the first field is the uid and the second is the name.
<joe9> uname glenda glenda  [21:02]
<joe9> is it a good idea to use the linux uid as the userid and keep the name the same as the linux user name?
<joe9> I do not like the idea of mounting the fs without any permission checking (-P)
<joe9> but, not sure if that is the only option.  [21:03]
<Fish> when you mount with v9fs, you can map local users to fossil users  [02:48]
<Fish> https://www.kernel.org/doc/Documentation/filesystems/9p.txt  [02:50]
<Fish> uname=glenda
<Fish> mount -t 9p -o tcp,trans=tcp,uname=glenda,port=1234 1.2.3.4 /n/fossil_mounted_as_glenda  [02:51]
<Fish> and if you want authentication as well, you have to authenticate with factotum first

backup
from https://marc.info/?l=9fans&m=115697470118616&w=2
<mycroftiv> i am talking about the backup systems you are investigating
<mycroftiv> not how the disks are partitioned and set up
<mycroftiv> what i believe is the best system if you have the resources available for
            it is based on /sys/src/cmd/venti/words/backup.example   [14:56]
<mycroftiv> ive modified that into my 'ventiprog' script and combined it with a system
            for tracking and storing fossil rootscores  [14:57]
<joe9> your setup makes sense and is the way to go.  [14:58]
<joe9> I do not have multiple machines yet and am just trying to mirror disks on the
       same machine.
<joe9> I could use wrarena and multiple venti servers.
<joe9> but, it might be easier to replicate the venti arenas and not worry about the
       fossil, index, etc.  [14:59]
<mycroftiv> you could make it so that you only 'activate' the 2nd venti server for the
            purpose of making your dialy backup, then kill it off
<joe9> yes, that is an option too.
<mycroftiv> you definitely dont want to replicate the fossil, you just want to track
            the rootscores
<mycroftiv> if you really just want to backup the arenas data, you should almost
            certainly use rdarena and not mirrorarenas  [15:00]
<joe9> I got that idea from this https://marc.info/?l=9fans&m=147750604517288&w=2
<joe9> and http://www.cespedes.org/blog/90/how-to-mirror-one-venti-to-another  [15:01]
<mycroftiv> but if youd like any 'recovery' process to be completely painless, using a
            2nd venti server (even if it is only running briefly) and progressive
            backup with wrarena makes things much easier
<joe9> I agree, using a 2nd venti server makes the recovery painless.
<joe9> no doubt about it.
<mycroftiv> that second link is not very good, the first is good  [15:02]
<mycroftiv> the second has some misunderstandings of how rdarena/wrarena work  [15:03]
<joe9> ok, Thanks.
<mycroftiv> in fact to copy one venti to another, you dont need rdarena at all, and
            you dont need to stop the servers at all
<mycroftiv> if you want to just back up arenas to static data files, use rdarena
<mycroftiv> if you want an instantly usable backup with minimal hassle, use wrarena to
            a running duplicate venti server with a system which tracks already
            written information so only copies new data  [15:05]
<mycroftiv> also make a system to track and preserve your fossil rootscores  [15:06]
<mycroftiv> those *can* be recovered from the venti data, but it is additional hassle
<mycroftiv> perhaps in your case it is easier to just use rdarena to dump arenas data
            to the backup drive, that just means that your recover process will be
            more elaborate compared to wrarena to a running backup venti  [15:09]
[mycroftiv]


<joe9> I like your approach of using venti/mirrorarenas to mirror the arenas to a different disk. But, I cannot find more details or help on
       venti/mirrorarenas. http://www.cespedes.org/blog/90/how-to-mirror-one-venti-to-another is the only information I could find it.  [14:39]
<joe9> The article talks about stopping the venti server whereas your email message made no mention of it https://marc.info/?l=9fans&m=147750604517288&w=2
<joe9> Just want to check how often you run the mirrorarenas? and, how you schedule it?  [14:40]
<Fish> it's quite easy  [15:03]
<Fish> just run venti/mirrorarenas /dev/sdE0/arenas /dev/sdE1/arenas
<Fish> and it will copy /dev/sdE0/arenas to /dev/sdE1/arenas
<Fish> this main drawback of this approach is that you need 2 identical arenas partitions
<Fish> I run venti/mirrorarenas in a cron job started every night  [15:04]
<joe9> Do you have to stop the venti service while running the mirror arenas?  [15:05]
<Fish> no
<joe9> ok, Thanks.
<Fish> you can even be writting on venti when it happens, it doesn't matter
<Fish> venti is append-only anyway, so it's always self-consistent  [15:06]
<joe9> to setup the second disk /dev/sdE1/arenas, it is just this command, correct? 	venti/fmtarenas arenas-sdE3 /dev/fs/arenas-sdE3  [15:07]
<joe9> s/sdE3/sdE1/  [15:08]
<joe9> the venti conf (which is irrelevant unless it is used to start a server) would the same as that on /dev/sdE0 except for 'arenas /dev/sdE1/arenas'
<joe9> venti/fmtarenas arenas-sdE3 /dev/fs/arenas-sdE3 -- using fs in this command as I am using cryptsetup, else, it would be /dev/sdE1/arenas  [15:12]
<Fish> I'm not sure "arenas-sdE3" is right, I think the arenas should be named the same as the partition you're mirroring from  [15:14]
<joe9> got it. my bad. Thanks.
<joe9> Do you bother with cryptsetup (specifically on the venti arenas)?  [15:17]
<Fish> I never tried cryptsetup with venti  [15:19]
<Fish> I leave my venti partitions unencrypted


to change the permissions of a directory from fossilconsole
:glenda@cirno:/usr/glenda; con /srv/fscons-sdE2
prompt: fsys main stat /active/usr/j
	stat /active/usr/j j 101 101 d700 0
prompt: fsys main wstat
usage: [fsys name] wstat file elem uid gid mode length
	use - for any field to mean don't change
prompt:  fsys main wstat /active/usr/j - - - d775 -
	old: wstat /active/usr/j j 101 101 d700 0
	new: wstat /active/usr/j j 101 101 d775 0
prompt: fsys main stat /active/usr/j
	stat /active/usr/j j 101 101 d775 0
prompt: attach main as joe: allowing as none
attach main as joe: allowing as none


removing users
from https://marc.info/?l=9fans&m=111558888121662&w=2

If you're running dumps, it's not a good idea to remove
users -- their ids will still be found looking in the dump.

You can rename the uname with 'uname old %new' but
you shouldn't get rid of the uid.

> so i should rename them all to some null user which is unable to log in?

Don't rename them all to the same thing.
That will just confuse you later.  We don't
ever remove people -- we just turn off their
accounts on the auth server.

We had a spell where we renamed foo to usedtobefoo,
but we've basically given up on that too.

removing users is always a bad idea.  once you blow them away you have
lost the trace of who they were, what they did and what they might be doing.

they should always be retired;  turn off their access but leave their
'existance' alone.

from https://marc.info/?l=9fans&m=111558888221673&w=2
the format of users file is:

	id:name:[leader]:[members[,members]]

	id is what is used to store in the file system
	name is what the outside world (9p) sees and deals with
	leader is the group leader (may be nil)
	members is a comma-separated list of who is in the group

a check is made when the users file is read by the server that
there is only one id<->name mapping.

so, say we have
	bob:bob::
and create some files. these files are on permanent storage in the
back end (venti, the original standalone fs, whatever) and the id 'bob'
is tied to those files irrevocably.

if bob leaves, we just need to turn off his ability to authenticate
to the server, there's no real need to remove his files and, in fact, those
files of his in the dump cannot be changed or removed.

the idea is that if a new bob arrives we can still give him the name bob
by changing the old bob entry to
	bob:bobwholeft::
(this is accomplished by using the 'uname' command in fossilcons(8), e.g.
	uname bob %bobwholeft
)
and then making a new entry
	newbob:bob::
via
	uname bob newbob

but there no way the new bob can access the old bob's files as they
are stored under different ids.

more fossil documentation
from https://marc.info/?l=9fans&m=111558922526314&w=2
wiki is at https://9p.io/wiki/plan9/plan_9_wiki/
wiki index https://9p.io/wiki/plan9/Wiki_index/index.html
old wiki https://9p.io/wiki/plan9/Old_wiki_pages/index.html


user authentication
from http://fqa.9front.org/fqa7.html#7.1.2
Note: Users seeking access to the file server must be added as a user
 on the file system itself, and, if auth is enabled, added to the auth
 server’s user database.
<joe9> just want to run this by you. For having a proper permission checking and
       authentication to work with fossil, the user needs to be setup in the
       authentication server too and the user (from plan9port) should be using
       factotum to authenticate. correct?  [11:06]
<joe9> I can always use none and disable permission checking, etc.
<joe9> but, the proper way is to use the authentication server + factotum + fossil
       user, correct?
<mycroftiv> yes, and you should make that a 'real' user - the thing where the user
            just exists in the users table on the file server but doesnt really exist
            is going to frustrate you  [11:07]
<joe9> ok, Thanks.
<mycroftiv> yes you can use plan9port factotum and p9sk1 auth with the plan9 auth
            server
<mycroftiv> i think i already explained though that p9sk1 is insecure now
<mycroftiv> that doesnt mean you 'shouldnt' use it - depends on your situation/threat
            model etc
<joe9> makes sense. Thanks.
<mycroftiv> but using p9sk1 auth between the systems should be thought of as a 'screen
            door latch' not a secure bank vault door
<mycroftiv> and also you will have some hassles to deal with
<mycroftiv> becuase 9front can do p9sk1 auth still, but it is now disabled by default
<mycroftiv> and working with p9p factotum has its own wrinkles
<mycroftiv> thats not a setup ive used in quite awhile so i cant help much with it,
            and i imagine cat-v isnt going to be very helpful about p9sk1 + p9p +
            fossil setups, that is like 3 things they hate on :)  [11:10]
<mycroftiv> but trying to get that set up and working is probably a good exercise, id
            work on it piece by piece  [11:14]
<mycroftiv> get auth server set up and working with dp9ik, re-enable p9sk1, then bring
            in p9p factotum

http://wildflower.diablonet.net/~scaron/p9setup.html
now we are ready to create user accounts. every time we create a user
the procedure is that we will first add them to the authentication
system with auth/changeuser, and then add them to the fileserver with
the commands,
con -l /srv/fscons
uname [user] [user]
uname sys +[user]

So, the process described above is:
1. Add user to authserver:
   7.4.2 - Adding users from http://fqa.9front.org/fqa7.html#7.4.2
2. Add user to fossil using the fscons and uname described above
3. remove -N flag in /rc/bin/service.auth/tcp567 to enable p9sk1 to work
   if not, this happens when srv tries to connect:
      plan9port/bin/srv: authproxy: auth_proxy rpc: p9any client get tickets: p9sk1: plan9port gettickets: remote: DES is disabled


connecting from plan9port
https://marc.info/?t=125437396200002&r=1&w=4 auth, connect and mount in plan9port

<joe9> Fish, I cannot get the fossil authentication to work with
       plan9port/bin/srv for my user.
<Fish> how do you use srv?  [15:41]
<Fish> you have to used factotum + srv -a to authenticate
<Fish> srv -a -k user=<user> <addr>
<Fish> 9mount -i unix\!$(namespace)/<addr> <mnt>  [15:43]
<joe9> Fish, this authenticates fine  [15:44]
<joe9> PLAN9=/home/j/dev/apps/plan9/plan9port
       PATH=$PATH:/home/j/dev/apps/plan9/plan9port/bin
       /home/j/dev/apps/plan9/plan9port/bin/srv -a -k user=glenda cirno:20564 fossil
<joe9> whereas this fails:
<joe9> PLAN9=/home/j/dev/apps/plan9/plan9port
       PATH=$PATH:/home/j/dev/apps/plan9/plan9port/bin
       /home/j/dev/apps/plan9/plan9port/bin/srv -a -k user=joe cirno:20564 fossil
<joe9> /home/j/dev/apps/plan9/plan9port/bin/srv: authproxy: auth_proxy rpc: p9any
       client get tickets: p9sk1: gettickets: unknown host 9front
<joe9> this http://okturing.com/src/2150/body  is from /lib/ndb/local  [15:45]
<joe9> I see the 'unknown host 9front' even when the user is glenda, but, it still
       authenticates.
<Fish> because glenda is root
<joe9> It fails when the user is joe though
<Fish> you didn't set ndb properly
<joe9> as an fyi, this is the full /lib/ndb/local http://okturing.com/src/2151/body
<joe9> what am I missing?
<Fish> you have to add a line "authdom=9front host=1.2.3.4" in you client's ndb
<Fish> otherwise, factotum couldn't figure what authentication server to call  [15:47]
<Fish> it works with glenda because it's obviously hostowner, so it bypasses the
       authentication server
<joe9> where is the 'client's ndb'? on the linux machine?
<Fish> $PLAN9/ndb
<joe9> oh, cool. That makes perfect sense.  [15:48]
<Fish> or, alternatively, make "9front" resolvable
<joe9> when you say $PLAN9, do you mean the 9front machine?  [15:50]
<joe9> I presume you mean where the plan9port is running, correct?
<Fish> yes
<Fish> usually /usr/local/plan9/ndb/local
<Fish> (I said $PLAN9/lib/ndb, but it's $PLAN9/ndb)
<Fish> $PLAN9/ndb/local
<joe9> oh, that i find.
<joe9> thanks
<joe9> cool, That worked. YOU ARE A GOD  [15:52]
<joe9> I am using factotum -Dd -a 192.168.0.5 -n , where 192.168.0.5 is the 9front
       machine which also has the authentication server. That machine had the authdom
       and auth set correctly.  [15:57]
<joe9> factotum still looks in the client ndb for the authentication server instead of
       using the -a provided authentication server
<joe9> is that expected?
<joe9> just trying to understand now that it works.  [15:58]
<Fish> you don't really need "-a"
<Fish> I suppose factotum tries the different options in a specific order  [16:01]
<Fish> which is something like
<Fish> 1. resolve dns
<Fish> 2. look ndb
<Fish> 3. use -a
<Fish> if it always used -a, it wouldn't be very practical to use
<joe9> makes sense. Thanks.  [16:03]
<joe9> when I do the srv with the user=joe .. , it prompts me for the password. Is
       there any way to avoid the prompt? read it from a file or such?  [16:05]
<joe9> it does it only for the first time so I presume that it is being cached
       somewhere.  [16:06]
<joe9> if I could add it to the cache (with a script), then I would not have to worry
       about the prompt.
<joe9> or, I could write an expect script wrapper.
<joe9> another quick question, for a smooth operation between linux and fossil, would
       you recommend the uname id of fossil users to be the uid of linux? or, just
       leave it to be the name?  [16:13]
<Fish> you should start factotum and fill it from secstore or a file  [16:26]
<Fish> for example  [16:27]
<Fish> factotum -n
<Fish> 9p write -l factotum/ctl < mypasswords
<Fish> where mypasswords is a file in factotum format filled with "key dom=<domain>
       proto=p9sk1 role=client user=<user> !password=<password>"  [16:28]
<Fish> also, if you start factotum, then authenticate, factotum will automatically
       store the login/password/domain in memory, so it won't be asked again as long
       as you use the same factotum instance  [16:29]
<Fish> I use the same login on linux and plan 9, but it don't think it matters much
<joe9> Thanks

<joe9> There is no need to map the unix user ids
       to fossil user names. the system is doing the translation.  [17:53]
<joe9> and, when I want just a fossil user without drawterm access, then I do not need
       the user in cwfs.  [17:54]
<joe9> just fossil and authsrv seem to do the trick.

PLAN9 usage
<joe9> Fish, just want to bring this to your attention
       http://okturing.com/src/2153/body  [21:40]
<joe9> if there is a prefix of PLAN9, srv works fine. If I declare PLAN9 earlier and
       then run the command without the PLAN9 prefix, it fails
<joe9> If you think it is a bug, I can add it to plan9port issues.
<joe9> Fish, not sure if you find this
       interesting. https://github.com/9fans/plan9port/issues/81  [22:15]
<joe9> when I mount using 9pfuse and try to chmod, I get this message : chmod:
       changing permissions of 'test.txt': Numerical result out of range  [22:16]
<joe9> it works fine when it is mounted with mount -t 9p though.
<Fish> there is something odd in the way you use PLAN9  [03:19]
<Fish> you should just export PLAN9 in your .bashrc/.bash_profile/.profile
<Fish> and add $PLAN9/bin to the end of your path  [03:20]
<Fish> and be done with it
<Fish> joe9: that's probably the same issue you get with 9pfuse, you should try to
       proposed change and let me know if it works  [03:21]
<joe9> Fish, ok. Thanks.
